import os
from deepeval import evaluate
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models.base_model import DeepEvalBaseLLM
import openai

# ä¸ºOpenAIè®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "sk-deepbank-dev"
os.environ["OPENAI_API_BASE"] = "https://litellm-dev.sandbox.deepbank.daikuan.qihoo.net/v1"

class CustomModel(DeepEvalBaseLLM):
    def __init__(self, model_name: str, api_key: str = None, base_url: str = None, temperature: float = 0, max_tokens: int = 1000):
        """
        è‡ªå®šä¹‰æ¨¡å‹ç±»ï¼Œæ”¯æŒä»»æ„æ¨¡å‹åç§°
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚ "qwen3-32b", "gpt-4", "claude-3" ç­‰
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            base_url: APIåŸºç¡€URLï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œé»˜è®¤0
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤1000
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œæ”¯æŒè‡ªå®šä¹‰å‚æ•°
        self.client = openai.OpenAI(
            api_key=api_key or os.getenv("OPENAI_API_KEY", "sk-deepbank-dev"),
            base_url=base_url or os.getenv("OPENAI_API_BASE", "https://litellm-dev.sandbox.deepbank.daikuan.qihoo.net/v1")
        )

    def load_model(self):
        return self.client

    def generate(self, prompt: str) -> str:
        client = self.load_model()
        response = client.chat.completions.create(
            model=self.model_name,  # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        return response.choices[0].message.content

    async def a_generate(self, prompt: str) -> str:
        # ç®€å•é‡ç”¨åŒæ­¥æ–¹æ³•ï¼Œæ‚¨ä¹Ÿå¯ä»¥å®ç°å¼‚æ­¥ç‰ˆæœ¬
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

def run_evaluation_and_print_results():
    """
    è¿è¡Œè¯„æµ‹å¹¶æ‰“å°ç»“æœçš„å‡½æ•°
    """
    # 1. åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹
    custom_model = CustomModel(
        model_name="qwen3-32b",
        temperature=0,
        max_tokens=1000
    )
    
    # 2. å®šä¹‰è¯„æµ‹æŒ‡æ ‡
    correctness_metric = GEval(
        name="Correctness",
        criteria="Determine if the 'actual output' is correct based on the 'expected output'. Provide detailed reasoning for the score.",
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5,
        model=custom_model
    )
    
    # 3. åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ - å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®æ–­ç‚¹æŸ¥çœ‹æµ‹è¯•æ•°æ®
    test_cases = [
        LLMTestCase(
            input="1+1=?",
            actual_output="3",  # é”™è¯¯ç­”æ¡ˆ
            expected_output="2",
        ),
        LLMTestCase(
            input="1+1=?", 
            actual_output="2",  # æ­£ç¡®ç­”æ¡ˆ
            expected_output="2",
        ),
        LLMTestCase(
            input="What is the capital of France?",
            actual_output="Paris",
            expected_output="Paris",
        ),
        LLMTestCase(
            input="What is the capital of France?",
            actual_output="London",  # é”™è¯¯ç­”æ¡ˆ
            expected_output="Paris",
        )
    ]
    
    # 4. è¿è¡Œè¯„æµ‹ - å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®æ–­ç‚¹æŸ¥çœ‹è¯„æµ‹è¿‡ç¨‹
    print("ğŸš€ å¼€å§‹è¿è¡Œè¯„æµ‹...")
    test_results = evaluate(test_cases, [correctness_metric])  # ğŸ” æ–­ç‚¹ä½ç½®1: è¯„æµ‹å¼€å§‹
    
    # 5. æ‰“å°è¯¦ç»†ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š è¯¦ç»†è¯„æµ‹ç»“æœ")
    print("="*60)
    
    passed_count = 0
    total_score = 0
    
    for i, test_result in enumerate(test_results):
        test_case = test_cases[i]
        metric_data = test_result.metrics_data[0]  # ğŸ” æ–­ç‚¹ä½ç½®2: æŸ¥çœ‹æ¯ä¸ªç»“æœ
        
        total_score += metric_data.score
        if metric_data.success:
            passed_count += 1
        
        # ğŸ” æ–­ç‚¹ä½ç½®3: æŸ¥çœ‹è¯¦ç»†çš„è¯„æµ‹æ•°æ®
        print(f"\nğŸ“ æµ‹è¯•ç”¨ä¾‹ {i+1}:")
        print(f"   é—®é¢˜: {test_case.input}")
        print(f"   å®é™…è¾“å‡º: {test_case.actual_output}")
        print(f"   æœŸæœ›è¾“å‡º: {test_case.expected_output}")
        print(f"   è¯„åˆ†: {metric_data.score:.2f}/1.0")
        print(f"   é˜ˆå€¼: {metric_data.threshold}")
        print(f"   ç»“æœ: {'âœ… é€šè¿‡' if metric_data.success else 'âŒ å¤±è´¥'}")
        print(f"   è¯„æµ‹åŸå› : {metric_data.reason}")
        
        if metric_data.error:
            print(f"   é”™è¯¯ä¿¡æ¯: {metric_data.error}")
    
    # 6. æ‰“å°æ€»ç»“ - ğŸ” æ–­ç‚¹ä½ç½®4: æŸ¥çœ‹æœ€ç»ˆç»Ÿè®¡
    total_tests = len(test_results)
    average_score = total_score / total_tests
    pass_rate = passed_count / total_tests
    
    print("\n" + "="*60)
    print("ğŸ“ˆ è¯„æµ‹æ€»ç»“")
    print("="*60)
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æ•°: {passed_count}")
    print(f"å¤±è´¥æ•°: {total_tests - passed_count}")
    print(f"é€šè¿‡ç‡: {pass_rate:.1%}")
    print(f"å¹³å‡å¾—åˆ†: {average_score:.2f}")
    print("="*60)

if __name__ == "__main__":
    run_evaluation_and_print_results()
    print("\nğŸ‰ è¯„æµ‹å®Œæˆï¼")