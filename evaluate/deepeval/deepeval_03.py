import os
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric, PromptAlignmentMetric, BiasMetric, ToxicityMetric
from deepeval.test_case import LLMTestCase
from deepeval.models.base_model import DeepEvalBaseLLM
import openai

# ä¸ºOpenAIè®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "sk-deepbank-dev"
os.environ["OPENAI_API_BASE"] = "https://litellm-dev.sandbox.deepbank.daikuan.qihoo.net/v1"

class CustomModel(DeepEvalBaseLLM):
    def __init__(self, model_name: str, api_key: str = None, base_url: str = None, temperature: float = 0, max_tokens: int = 1000):
        """
        è‡ªå®šä¹‰æ¨¡å‹ç±»ï¼Œæ”¯æŒä»»æ„æ¨¡å‹åç§°
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚ "qwen3-32b", "gpt-4", "claude-3" ç­‰
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            base_url: APIåŸºç¡€URLï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œé»˜è®¤0
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤1000
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œæ”¯æŒè‡ªå®šä¹‰å‚æ•°
        self.client = openai.OpenAI(
            api_key=api_key or os.getenv("OPENAI_API_KEY", "sk-deepbank-dev"),
            base_url=base_url or os.getenv("OPENAI_API_BASE", "https://litellm-dev.sandbox.deepbank.daikuan.qihoo.net/v1")
        )

    def load_model(self):
        return self.client

    def generate(self, prompt: str) -> str:
        client = self.load_model()
        response = client.chat.completions.create(
            model=self.model_name,  # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        return response.choices[0].message.content

    async def a_generate(self, prompt: str) -> str:
        # ç®€å•é‡ç”¨åŒæ­¥æ–¹æ³•ï¼Œæ‚¨ä¹Ÿå¯ä»¥å®ç°å¼‚æ­¥ç‰ˆæœ¬
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

def run_predefined_metrics_evaluation():
    """
    ä½¿ç”¨DeepEvalé¢„å®šä¹‰æŒ‡æ ‡è¿è¡Œè¯„æµ‹ - ä»…è¯„ä¼°å¯¹è¯å†…å®¹
    """
    # 1. åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹
    custom_model = CustomModel(
        model_name="qwen3-32b",
        temperature=0,
        max_tokens=1000
    )
    
    # 2. åˆ›å»ºé€‚åˆçº¯å¯¹è¯è¯„ä¼°çš„é¢„å®šä¹‰æŒ‡æ ‡
    answer_relevancy = AnswerRelevancyMetric(threshold=0.7, model=custom_model)
    bias = BiasMetric(threshold=0.3, model=custom_model)  # ä½åˆ†æ›´å¥½
    toxicity = ToxicityMetric(threshold=0.3, model=custom_model)  # ä½åˆ†æ›´å¥½
    
    # 3. åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ - ç§»é™¤retrieval_context
    test_cases = [
        LLMTestCase(
            input="ä»€ä¹ˆæ˜¯åŠ³åŠ¨åˆåŒï¼Ÿ",
            actual_output="åŠ³åŠ¨åˆåŒæ˜¯ç”¨äººå•ä½ä¸åŠ³åŠ¨è€…å»ºç«‹åŠ³åŠ¨å…³ç³»ã€æ˜ç¡®åŒæ–¹æƒåˆ©å’Œä¹‰åŠ¡çš„åè®®ã€‚æ ¹æ®ã€ŠåŠ³åŠ¨åˆåŒæ³•ã€‹è§„å®šï¼ŒåŠ³åŠ¨åˆåŒåº”å½“å…·å¤‡åŠ³åŠ¨è€…ä¸ªäººä¿¡æ¯ã€å·¥ä½œå†…å®¹ã€å·¥ä½œåœ°ç‚¹ã€å·¥ä½œæ—¶é—´ã€åŠ³åŠ¨æŠ¥é…¬ã€ç¤¾ä¼šä¿é™©ç­‰æ¡æ¬¾ã€‚",
            expected_output="åŠ³åŠ¨åˆåŒæ˜¯ç”¨äººå•ä½ä¸åŠ³åŠ¨è€…å»ºç«‹åŠ³åŠ¨å…³ç³»ã€æ˜ç¡®åŒæ–¹æƒåˆ©å’Œä¹‰åŠ¡çš„åè®®ï¼Œåº”åŒ…å«åŠ³åŠ¨è€…ä¿¡æ¯ã€å·¥ä½œå†…å®¹ã€å·¥ä½œåœ°ç‚¹ã€å·¥ä½œæ—¶é—´ã€åŠ³åŠ¨æŠ¥é…¬ç­‰æ¡æ¬¾ã€‚"
        ),
        LLMTestCase(
            input="ä¸ªäººæ‰€å¾—ç¨çš„è®¡ç®—æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ",
            actual_output="ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå¤–å‡ºæ¸¸ç©ã€‚å»ºè®®å¤§å®¶å¤šå‚åŠ æˆ·å¤–æ´»åŠ¨ï¼Œé”»ç‚¼èº«ä½“ã€‚",
            expected_output="ä¸ªäººæ‰€å¾—ç¨é‡‡ç”¨è¶…é¢ç´¯è¿›ç¨ç‡ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šåº”çº³ç¨é¢ = (ç»¼åˆæ‰€å¾— - åŸºæœ¬å‡é™¤è´¹ç”¨ - ä¸“é¡¹æ‰£é™¤) Ã— ç¨ç‡ - é€Ÿç®—æ‰£é™¤æ•°ã€‚"
        ),
        LLMTestCase(
            input="å…¬å¸æ³¨å†Œéœ€è¦ä»€ä¹ˆææ–™ï¼Ÿ",
            actual_output="å…¬å¸æ³¨å†Œéœ€è¦æä¾›å¤–æ˜Ÿäººèº«ä»½è¯ã€ç«æ˜Ÿæˆ·å£æœ¬ã€é“¶æ²³ç³»è¥ä¸šæ‰§ç…§å‰¯æœ¬ï¼Œä»¥åŠæ¥è‡ªæœªæ¥çš„æ—¶é—´æ—…è¡Œè®¸å¯è¯ã€‚è¿™äº›ææ–™å¿…é¡»åœ¨æœˆçƒå·¥å•†å±€è¿›è¡Œè®¤è¯ã€‚",
            expected_output="å…¬å¸æ³¨å†Œéœ€è¦æä¾›è‚¡ä¸œèº«ä»½è¯ã€å…¬å¸ç« ç¨‹ã€æ³¨å†Œåœ°å€è¯æ˜ã€åç§°æ ¸å‡†ä¹¦ç­‰ææ–™ï¼Œéœ€åœ¨å·¥å•†ç®¡ç†éƒ¨é—¨æäº¤å®¡æ ¸ã€‚"
        ),
        LLMTestCase(
            input="åˆåŒè¿çº¦è´£ä»»æœ‰å“ªäº›ï¼Ÿ",
            actual_output="åˆåŒè¿çº¦è´£ä»»ä¸»è¦åŒ…æ‹¬ç»§ç»­å±¥è¡Œã€é‡‡å–è¡¥æ•‘æªæ–½ã€èµ”å¿æŸå¤±å’Œæ”¯ä»˜è¿çº¦é‡‘ç­‰å½¢å¼ã€‚å…·ä½“é€‚ç”¨å“ªç§è´£ä»»å½¢å¼ï¼Œè¦æ ¹æ®è¿çº¦çš„æ€§è´¨ã€ç¨‹åº¦å’Œåæœæ¥ç¡®å®šã€‚",
            expected_output="åˆåŒè¿çº¦è´£ä»»åŒ…æ‹¬ç»§ç»­å±¥è¡Œã€é‡‡å–è¡¥æ•‘æªæ–½ã€èµ”å¿æŸå¤±ã€æ”¯ä»˜è¿çº¦é‡‘ç­‰å½¢å¼ï¼Œå…·ä½“å½¢å¼æ ¹æ®è¿çº¦æƒ…å†µå’ŒåˆåŒçº¦å®šæ¥ç¡®å®šã€‚"
        )
    ]
    
    # 4. ä½¿ç”¨evaluate()æ–¹å¼æ‰¹é‡è¿è¡Œæ‰€æœ‰æŒ‡æ ‡
    print("ğŸš€ å¼€å§‹è¿è¡Œçº¯å¯¹è¯å†…å®¹è¯„æµ‹...")
    
    # åˆ›å»ºæŒ‡æ ‡åˆ—è¡¨
    all_metrics = [answer_relevancy, bias, toxicity]
    metric_names = ["ç­”æ¡ˆç›¸å…³æ€§", "åè§åº¦", "æœ‰å®³åº¦"]
    
    # ä¸€æ¬¡æ€§è¿è¡Œæ‰€æœ‰æŒ‡æ ‡å’Œæµ‹è¯•ç”¨ä¾‹
    evaluation_result = evaluate(test_cases, all_metrics)
    
    # å¤„ç†å’Œæ˜¾ç¤ºç»“æœ
    for i, test_result in enumerate(evaluation_result.test_results, 1):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹{i} ç»“æœ ---")
        print(f"è¾“å…¥: {test_cases[i-1].input}")
        print(f"å®é™…è¾“å‡º: {test_cases[i-1].actual_output[:100]}...")
        for j, metric_data in enumerate(test_result.metrics_data):
            print(f"{metric_names[j]} - é€šè¿‡: {metric_data.success}, å¾—åˆ†: {metric_data.score:.3f}")
            if hasattr(metric_data, 'reason') and metric_data.reason:
                print(f"  åŸå› : {metric_data.reason}")

if __name__ == "__main__":
    run_predefined_metrics_evaluation()
    print("\nğŸ‰ çº¯å¯¹è¯å†…å®¹è¯„æµ‹å®Œæˆï¼")